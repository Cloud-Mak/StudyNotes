LXC-LXD test

Checking group details
makrand@mint-gl63:~$ getent group lxd
lxd:x:999:

makrand@mint-gl63:~$ sudo gpasswd -a makrand lxd
Adding user makrand to group lxd

List all images
 lxc image list images: | grep <name>

 lxc image list images: |\
 awk -F'|' '{ print $2}' |\
 sed '/^[[:space:]]*$/d' |\
 awk -F'/' '{ print $1"/"$2 }' | sort | uniq | egrep -v 'more|ALIAS'

lxc alias (command alias)
lxc alias add list "list -c ns4pN,volatile.eth0.hwaddr:MAC,P"
(display container name,state,IPV4,PID,No of Process,MAC & profile)


makrand@mint-gl63:~$ lxc launch images:centos/7 kmaster --profile k8

makrand@mint-gl63:~$ lxc image alias create cent7 497149ac07d6



=> Run dokcer inside container
lxc config set <container> security.nesting true



https://uk.images.linuxcontainers.org/streams/v1/index.json

makrand@mint-gl63:~$ groups (to check groups user belong to)


makrand@mint-gl63:~$ lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: 
Name of the new storage pool [default=default]: 
Name of the storage backend to use (btrfs, ceph, dir, lvm, zfs) [default=zfs]: 
Create a new ZFS pool? (yes/no) [default=yes]: 
Would you like to use an existing block device? (yes/no) [default=no]: 
Size in GB of the new loop device (1GB minimum) [default=21GB]: 11GB
Would you like to connect to a MAAS server? (yes/no) [default=no]: 
Would you like to create a new local network bridge? (yes/no) [default=yes]: 
What should the new bridge be called? [default=lxdbr0]: 
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
Would you like LXD to be available over the network? (yes/no) [default=no]: 
Would you like stale cached images to be updated automatically? (yes/no) [default=yes] 
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]: yes
config: {}
networks:
- config:
    ipv4.address: auto
    ipv6.address: auto
  description: ""
  managed: false
  name: lxdbr0
  type: ""
storage_pools:
- config:
    size: 11GB
  description: ""
  name: default
  driver: zfs
profiles:
- config: {}
  description: ""
  devices:
    eth0:
      name: eth0
      nictype: bridged
      parent: lxdbr0
      type: nic
    root:
      path: /
      pool: default
      type: disk
  name: default
cluster: null


Error: Failed instance creation: read tcp 10.100.100.111:53758->91.189.88.37:443: read: connection timed out


# Launch containers

lxc launch ubuntu:18.04 <Container-Name>
lxc launch ubuntu-daily:18.04 <Container-Name>
lxc launch images:centos/7/amd64 my-cent7



Password Less Login from host to lxd container - 
 Create container
 Edit sshd to allow root login with password
 ssh-copy-id -f -i /home/makrand/.ssh/id_rsa.pub root@10.70.241.97


^_^ Forward port from host to container

lxc config device add <CONTAINER> <DEVNAME> proxy listen=tcp:0.0.0.0:80 connect=tcp:127.0.0.1:80





========>> Clustering <<========

The cluster uses a distributed database to store its state. All nodes in the cluster need to access such distributed database in order to serve user requests. 
Every LXD cluster has up to 3 members that serve as database nodes
If the cluster has many nodes, only some of them will be picked to replicate database data. Each node that is picked can replicate data either as "voter" or as "stand-by"
The default number of voting nodes is 3 and the default number of stand-by nodes is 2. This means that your cluster will remain operation as long as you switch off at most one voting node at a time.
At each time there will be an elected cluster leader that will monitor the health of the other nodes. If a node is down for more than 20 seconds, its status will be marked as OFFLINE.
To upgrade a single node, simply upgrade the lxd/lxc binaries on the host (via snap or other packaging systems) and restart the lxd daemon.
As you proceed upgrading the rest of the nodes, they will all transition to the Blocked state, until you upgrade the very last one. At that point the blocked nodes will notice that there is no out-of-date node left and will become operational again.
If you have a 3-member cluster and you lose 2 members, the cluster will become unavailable. However, if at least one database node has survived, you will be able to recover the cluster.


#> cluster CLI 
 lxc config set cluster.max_voters <n>
 lxc config set cluster.max_standby <n>
 lxc cluster remove <node name>
 lxc config set cluster.offline_threshold <n seconds>  (min value should be 10)
 lxc cluster remove --force <node name>
 lxd cluster list-database
 lxc config set cluster.images_minimal_replica 1 (if set to -1 then image present in all)
 